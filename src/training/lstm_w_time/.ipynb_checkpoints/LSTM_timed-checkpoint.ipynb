{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_colab_env = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if google_colab_env:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    !git clone https://github.com/GrzegorzKazana/artificial-music.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## importing dataset, splitting tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'found 250 tracks'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "proj_base_path = ('/content/artificial-music' \n",
    "                  if google_colab_env else '../../../')\n",
    "\n",
    "data_base_path =  ('/content/drive/My Drive/artificial-music/datasets'\n",
    "                   if google_colab_env else '../../../datasets')\n",
    "\n",
    "models_base_path =  ('/content/drive/My Drive/artificial-music/pretrained_models' \n",
    "                     if google_colab_env else '../../../pretrained_models')\n",
    "\n",
    "sys.path.append(os.path.join(os.getcwd(), proj_base_path))\n",
    "\n",
    "dataset_path = 'numpy/pokemon_embedded_w_time/'\n",
    "dataset_tracks_dir = 'w_time'\n",
    "word_vectors_file = 'just_sparse/meta/_word_vectors_10000_ignore_ratio=0.05.wv'\n",
    "\n",
    "tracks_path = os.path.join(data_base_path, dataset_path, dataset_tracks_dir)\n",
    "word_vectors_path = os.path.join(data_base_path, dataset_path, word_vectors_file)\n",
    "\n",
    "track_paths = [os.path.join(tracks_path, f) for f in os.listdir(tracks_path)]\n",
    "\n",
    "f'found {len(track_paths)} tracks'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(303, 19)\n",
      "(1109, 19)\n",
      "(489, 19)\n",
      "(374, 19)\n",
      "(1109, 19)\n",
      "(488, 19)\n",
      "(210, 19)\n",
      "(489, 19)\n",
      "(489, 19)\n",
      "(210, 19)\n",
      "(678, 19)\n",
      "(1109, 19)\n",
      "(488, 19)\n",
      "(489, 19)\n",
      "(374, 19)\n",
      "(1109, 19)\n",
      "(303, 19)\n",
      "(505, 19)\n",
      "(210, 19)\n",
      "(505, 19)\n",
      "(303, 19)\n",
      "(374, 19)\n",
      "(489, 19)\n",
      "(1109, 19)\n",
      "(678, 19)\n",
      "(489, 19)\n",
      "(555, 19)\n",
      "(555, 19)\n",
      "(489, 19)\n",
      "(210, 19)\n",
      "(678, 19)\n",
      "(1109, 19)\n",
      "(374, 19)\n",
      "(489, 19)\n",
      "(303, 19)\n",
      "(505, 19)\n",
      "(1109, 19)\n",
      "(210, 19)\n",
      "(505, 19)\n",
      "(303, 19)\n",
      "(1109, 19)\n",
      "(489, 19)\n",
      "(374, 19)\n",
      "(153, 19)\n",
      "(210, 19)\n",
      "(489, 19)\n",
      "(210, 19)\n",
      "(489, 19)\n",
      "(374, 19)\n",
      "(1109, 19)\n",
      "(303, 19)\n",
      "(505, 19)\n",
      "(505, 19)\n",
      "(303, 19)\n",
      "(1109, 19)\n",
      "(374, 19)\n",
      "(489, 19)\n",
      "(153, 19)\n",
      "(210, 19)\n",
      "(374, 19)\n",
      "(489, 19)\n",
      "(489, 19)\n",
      "(374, 19)\n",
      "(210, 19)\n",
      "(153, 19)\n",
      "(374, 19)\n",
      "(489, 19)\n",
      "(1109, 19)\n",
      "(303, 19)\n",
      "(505, 19)\n",
      "(153, 19)\n",
      "(678, 19)\n",
      "(303, 19)\n",
      "(374, 19)\n",
      "(505, 19)\n",
      "(488, 19)\n",
      "(488, 19)\n",
      "(505, 19)\n",
      "(374, 19)\n",
      "(678, 19)\n",
      "(153, 19)\n",
      "(678, 19)\n",
      "(303, 19)\n",
      "(555, 19)\n",
      "(505, 19)\n",
      "(488, 19)\n",
      "(488, 19)\n",
      "(505, 19)\n",
      "(555, 19)\n",
      "(303, 19)\n",
      "(374, 19)\n",
      "(678, 19)\n",
      "(488, 19)\n",
      "(153, 19)\n",
      "(678, 19)\n",
      "(678, 19)\n",
      "(303, 19)\n",
      "(555, 19)\n",
      "(505, 19)\n",
      "(488, 19)\n",
      "(488, 19)\n",
      "(505, 19)\n",
      "(555, 19)\n",
      "(303, 19)\n",
      "(678, 19)\n",
      "(303, 19)\n",
      "(153, 19)\n",
      "(678, 19)\n",
      "(488, 19)\n",
      "(303, 19)\n",
      "(1109, 19)\n",
      "(678, 19)\n",
      "(303, 19)\n",
      "(505, 19)\n",
      "(488, 19)\n",
      "(488, 19)\n",
      "(505, 19)\n",
      "(555, 19)\n",
      "(303, 19)\n",
      "(678, 19)\n",
      "(1109, 19)\n",
      "(303, 19)\n",
      "(488, 19)\n",
      "(678, 19)\n",
      "(153, 19)\n",
      "(1109, 19)\n",
      "(303, 19)\n",
      "(489, 19)\n",
      "(555, 19)\n",
      "(488, 19)\n",
      "(488, 19)\n",
      "(555, 19)\n",
      "(303, 19)\n",
      "(1109, 19)\n",
      "(678, 19)\n",
      "(153, 19)\n",
      "(153, 19)\n",
      "(153, 19)\n",
      "(678, 19)\n",
      "(153, 19)\n",
      "(1109, 19)\n",
      "(489, 19)\n",
      "(505, 19)\n",
      "(555, 19)\n",
      "(555, 19)\n",
      "(505, 19)\n",
      "(489, 19)\n",
      "(1109, 19)\n",
      "(153, 19)\n",
      "(153, 19)\n",
      "(678, 19)\n",
      "(678, 19)\n",
      "(678, 19)\n",
      "(153, 19)\n",
      "(153, 19)\n",
      "(1109, 19)\n",
      "(505, 19)\n",
      "(678, 19)\n",
      "(555, 19)\n",
      "(210, 19)\n",
      "(210, 19)\n",
      "(555, 19)\n",
      "(678, 19)\n",
      "(1109, 19)\n",
      "(153, 19)\n",
      "(153, 19)\n",
      "(678, 19)\n",
      "(678, 19)\n",
      "(153, 19)\n",
      "(153, 19)\n",
      "(678, 19)\n",
      "(505, 19)\n",
      "(1109, 19)\n",
      "(210, 19)\n",
      "(555, 19)\n",
      "(555, 19)\n",
      "(505, 19)\n",
      "(153, 19)\n",
      "(678, 19)\n",
      "(153, 19)\n",
      "(505, 19)\n",
      "(210, 19)\n",
      "(374, 19)\n",
      "(374, 19)\n",
      "(555, 19)\n",
      "(555, 19)\n",
      "(374, 19)\n",
      "(374, 19)\n",
      "(488, 19)\n",
      "(210, 19)\n",
      "(505, 19)\n",
      "(303, 19)\n",
      "(210, 19)\n",
      "(488, 19)\n",
      "(374, 19)\n",
      "(210, 19)\n",
      "(374, 19)\n",
      "(555, 19)\n",
      "(374, 19)\n",
      "(210, 19)\n",
      "(488, 19)\n",
      "(210, 19)\n",
      "(303, 19)\n",
      "(210, 19)\n",
      "(374, 19)\n",
      "(1109, 19)\n",
      "(488, 19)\n",
      "(210, 19)\n",
      "(505, 19)\n",
      "(489, 19)\n",
      "(374, 19)\n",
      "(489, 19)\n",
      "(555, 19)\n",
      "(555, 19)\n",
      "(489, 19)\n",
      "(303, 19)\n",
      "(374, 19)\n",
      "(153, 19)\n",
      "(555, 19)\n",
      "(488, 19)\n",
      "(488, 19)\n",
      "(374, 19)\n",
      "(1109, 19)\n",
      "(210, 19)\n",
      "(210, 19)\n",
      "(489, 19)\n",
      "(1109, 19)\n",
      "(488, 19)\n",
      "(488, 19)\n",
      "(555, 19)\n",
      "(210, 19)\n",
      "(505, 19)\n",
      "(153, 19)\n",
      "(489, 19)\n",
      "(303, 19)\n",
      "(374, 19)\n",
      "(555, 19)\n",
      "(555, 19)\n",
      "(303, 19)\n",
      "(489, 19)\n",
      "(153, 19)\n",
      "(505, 19)\n",
      "(210, 19)\n",
      "(555, 19)\n",
      "(488, 19)\n",
      "(1109, 19)\n",
      "(488, 19)\n",
      "(1109, 19)\n",
      "(489, 19)\n",
      "(210, 19)\n"
     ]
    }
   ],
   "source": [
    "# load tracks\n",
    "tracks = [np.load(p) for p in track_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load word vectors\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "wv = KeyedVectors.load(word_vectors_path, mmap='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5, 39, 19), (5, 39, 19))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def dataset_gen(tracks, window_size_range=(20, 300), batch_size=16):\n",
    "    \"\"\"\n",
    "    tracks - list of np.arrays of shape (track_length, frame_size)\n",
    "    window_size - length of generated batch\n",
    "    batch_size - number of sequences in batch\n",
    "    \"\"\"\n",
    "    max_window_size = min([len(t) for t in tracks]) - 3\n",
    "    while True:\n",
    "        window_size = np.random.randint(window_size_range[0], min(max_window_size, window_size_range[1]))\n",
    "        # select #batch_size tracks\n",
    "        selected_track_indicies = [np.random.randint(0, len(tracks)) for _ in range(batch_size)]\n",
    "        # select sequence starting point for each track\n",
    "        sequence_indicies = [np.random.randint(0, len(tracks[sti]) - window_size - 2)\n",
    "                             for sti in selected_track_indicies]\n",
    "        \n",
    "        \n",
    "        # create slices for x and y\n",
    "        x_slice = lambda seqi: np.s_[seqi:seqi + window_size]\n",
    "        y_slice = lambda seqi: np.s_[seqi + 1:seqi + window_size + 1]\n",
    "        \n",
    "        x = [tracks[sti][x_slice(seqi)] for sti, seqi in zip(selected_track_indicies, sequence_indicies)]\n",
    "        y = [tracks[sti][y_slice(seqi)] for sti, seqi in zip(selected_track_indicies, sequence_indicies)]\n",
    "\n",
    "        yield np.stack(x), np.stack(y)\n",
    "        \n",
    "x, y = next(dataset_gen(tracks, (10, 50), 5))\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras as K\n",
    "\n",
    "WORD_VECTOR_SIZE = 16\n",
    "REST_DATA = 3\n",
    "\n",
    "INPUT_SIZE = WORD_VECTOR_SIZE + REST_DATA\n",
    "HIDDEN_SIZE = 512\n",
    "HIDDEN_DENSE = 128\n",
    "\n",
    "NOTES_LSTM_SIZE = 128\n",
    "NOTES_DENSE_SIZE = 64\n",
    "\n",
    "REST_LSTM_SIZE = 32\n",
    "REST_DENSE_SIZE = 16\n",
    "\n",
    "OUTPUT_SIZE = INPUT_SIZE\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "WINDOW_SIZE_RANGE = (15, 100)\n",
    "\n",
    "INPUT_SHAPE = (None, INPUT_SIZE)\n",
    "# None allows for variable seq_length between batches\n",
    "\n",
    "NOTE_VECTOR_OUTPUT_NAME = 'note_vector'\n",
    "REST_DATA_OUTPUT_NAME = 'rest_data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load existing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or load saved model\n",
    "model_path = 'lstm_lstm/embedded_16_128_stacked_32/embedded_16_128_stacked_32md_e200_t2019-10-09T09_59_31_cpu.h5'\n",
    "model = K.models.load_model(os.path.join(models_base_path, model_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### or create new one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/grzegorzkazana/.local/share/virtualenvs/artificial-music-0CJsFWUp/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3326, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-23-0757024d27f7>\", line 1, in <module>\n",
      "    from tensorflow.keras.layers import Input, Dense, LSTM, CuDNNLSTM\n",
      "ImportError: cannot import name 'CuDNNLSTM' from 'tensorflow.keras.layers' (/Users/grzegorzkazana/.local/share/virtualenvs/artificial-music-0CJsFWUp/lib/python3.7/site-packages/tensorflow_core/python/keras/api/_v2/keras/layers/__init__.py)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/grzegorzkazana/.local/share/virtualenvs/artificial-music-0CJsFWUp/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2040, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'ImportError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/grzegorzkazana/.local/share/virtualenvs/artificial-music-0CJsFWUp/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/Users/grzegorzkazana/.local/share/virtualenvs/artificial-music-0CJsFWUp/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 319, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/Users/grzegorzkazana/.local/share/virtualenvs/artificial-music-0CJsFWUp/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 353, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/usr/local/Cellar/python/3.7.4/Frameworks/Python.framework/Versions/3.7/lib/python3.7/inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/usr/local/Cellar/python/3.7.4/Frameworks/Python.framework/Versions/3.7/lib/python3.7/inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/usr/local/Cellar/python/3.7.4/Frameworks/Python.framework/Versions/3.7/lib/python3.7/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/usr/local/Cellar/python/3.7.4/Frameworks/Python.framework/Versions/3.7/lib/python3.7/inspect.py\", line 733, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "  File \"/Users/grzegorzkazana/.local/share/virtualenvs/artificial-music-0CJsFWUp/lib/python3.7/site-packages/tensorflow/__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"/Users/grzegorzkazana/.local/share/virtualenvs/artificial-music-0CJsFWUp/lib/python3.7/site-packages/tensorflow/__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"/Users/grzegorzkazana/.local/share/virtualenvs/artificial-music-0CJsFWUp/lib/python3.7/importlib/__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 965, in _find_and_load_unlocked\n",
      "ModuleNotFoundError: No module named 'tensorflow_core.compat'\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'CuDNNLSTM' from 'tensorflow.keras.layers' (/Users/grzegorzkazana/.local/share/virtualenvs/artificial-music-0CJsFWUp/lib/python3.7/site-packages/tensorflow_core/python/keras/api/_v2/keras/layers/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, LSTM, CuDNNLSTM\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "rnn_layer = CuDNNLSTM if google_colab_env else LSTM\n",
    "\n",
    "input_layer = Input(shape=INPUT_SHAPE)\n",
    "\n",
    "x = rnn_layer(\n",
    "    HIDDEN_SIZE, \n",
    "    return_sequences=True,\n",
    ")(input_layer)\n",
    "x = Dense(HIDDEN_DENSE)(x)\n",
    "\n",
    "# no activation - regression task\n",
    "x1 = rnn_layer(NOTES_LSTM_SIZE, return_sequences=True)(x)\n",
    "x1 = Dense(NOTES_DENSE_SIZE)(x1)\n",
    "note_vector_output = Dense(WORD_VECTOR_SIZE, name=NOTE_VECTOR_OUTPUT_NAME)(x1)\n",
    "\n",
    "# relu - encoded rest data - timing and velocity must be positive\n",
    "x2 = rnn_layer(REST_LSTM_SIZE, return_sequences=True)(x)\n",
    "x2 = Dense(REST_DENSE_SIZE)(x2)\n",
    "rest_data_output = Dense(REST_DATA, activation='relu', name=REST_DATA_OUTPUT_NAME)(x2)\n",
    "\n",
    "model = Model(\n",
    "    inputs=input_layer, \n",
    "    outputs=[note_vector_output, rest_data_output]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = {\n",
    "    NOTE_VECTOR_OUTPUT_NAME: 'mse',\n",
    "    REST_DATA_OUTPUT_NAME: 'mse',\n",
    "}\n",
    "\n",
    "# balancing losses - output with less neurons needs larger weight\n",
    "output_weights = {\n",
    "    NOTE_VECTOR_OUTPUT_NAME: (1 - WORD_VECTOR_SIZE / (WORD_VECTOR_SIZE + REST_DATA)),\n",
    "    REST_DATA_OUTPUT_NAME: 5 * (1 - REST_DATA / (WORD_VECTOR_SIZE + REST_DATA)),\n",
    "}\n",
    "\n",
    "model.compile(\n",
    "    loss=losses,\n",
    "    loss_weights=output_weights,\n",
    "    optimizer='adam', \n",
    "    metrics=[\"mean_squared_error\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### define training callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.training.common.training_callbacks import ModelAndLogSavingCallback, GeneratingAndPlottingCallback\n",
    "\n",
    "# logging callback\n",
    "logging_path = 'lstm_w_time'\n",
    "experiment_name = f'embedded_w_time_{INPUT_SIZE}_{HIDDEN_SIZE}'\n",
    "experiment_path = os.path.join(models_base_path, logging_path, experiment_name)\n",
    "os.makedirs(experiment_path, exist_ok=True)\n",
    "print(f'saving checkpoints and logs to {experiment_path}')\n",
    "\n",
    "# logging disabled for now\n",
    "log_callback = ModelAndLogSavingCallback(model, experiment_path, save_log_only=True)\n",
    "\n",
    "# generating callback\n",
    "from src.generating.generating import recurrent_generate\n",
    "from src.generating.embedded_w_time_generating_seeds import seed_generators\n",
    "from src.data_processing.common.helpers import pipe\n",
    "from src.data_processing.embedded_with_time.unembed_in_time import np2sparse\n",
    "\n",
    "# note, that seed length refers to number of notes\n",
    "# instead of number of frames\n",
    "SEED_LENGTH = 10\n",
    "GENERATED_SEQ_LENGTH = 100\n",
    "GENERATING_WINDOW_SIZE = 10\n",
    "METHOD = 'multi_note_harmonic_seed'\n",
    "\n",
    "seed_generator = lambda: seed_generators[METHOD](\n",
    "    SEED_LENGTH, WORD_VECTOR_SIZE, word_vectors=wv, batch_size=BATCH_SIZE)\n",
    "\n",
    "sample_generator = lambda model, seed: recurrent_generate(\n",
    "    model, \n",
    "    seed, \n",
    "    GENERATED_SEQ_LENGTH, \n",
    "    GENERATING_WINDOW_SIZE, \n",
    "    is_binary=False, \n",
    "    transform_output=lambda y: np.concatenate((y[0], y[1]), axis=2)\n",
    ")\n",
    "\n",
    "sparse_sample_generator = lambda model, seed: pipe(\n",
    "    sample_generator(model, seed),\n",
    "    lambda batch_of_samples: [np2sparse(sample, wv) for sample in batch_of_samples]\n",
    ")\n",
    "\n",
    "print(f'generating sequences of {GENERATED_SEQ_LENGTH} using {METHOD}')\n",
    "\n",
    "gen_callback = GeneratingAndPlottingCallback(model, sparse_sample_generator, seed_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "mc = ModelCheckpoint(\n",
    "    os.path.join(experiment_path, 'm_e_{epoch:03d}-l_{val_loss:.2f}.h5'),\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_best_only=True,\n",
    ")\n",
    "\n",
    "es = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta=1e-2,\n",
    "    patience=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(253, 19)\n"
     ]
    }
   ],
   "source": [
    "# pre training code\n",
    "from time import time\n",
    "epochs_elapsed = 0\n",
    "minutes_elapsed = 0\n",
    "\n",
    "def split_output_gen(X, window_size_range, batch_size):\n",
    "    gen = dataset_gen(X, window_size_range, batch_size)\n",
    "    while True:\n",
    "        x, y = next(gen)\n",
    "        y_split = {\n",
    "            NOTE_VECTOR_OUTPUT_NAME: y[:, :, :WORD_VECTOR_SIZE],\n",
    "            REST_DATA_OUTPUT_NAME: y[:, :, -REST_DATA:],\n",
    "        }\n",
    "        \n",
    "        yield x, y_split\n",
    "\n",
    "# take 50 last notes from each track for validation\n",
    "VALIDATION_STEPS = 50\n",
    "train_tracks = [t[:-VALIDATION_STEPS, :] for t in tracks]\n",
    "validation_tracks = [t[-VALIDATION_STEPS:, :] for t in tracks]\n",
    "        \n",
    "data_gen = split_output_gen(train_tracks, WINDOW_SIZE_RANGE, BATCH_SIZE)\n",
    "test_gen = split_output_gen(validation_tracks, WINDOW_SIZE_RANGE, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "STEPS_PER_EPOCH = 1000\n",
    "TEST_STEPS = 100\n",
    "\n",
    "start_time = time()\n",
    "model.fit_generator(\n",
    "    data_gen,\n",
    "    steps_per_epoch=STEPS_PER_EPOCH,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=test_gen,\n",
    "    validation_steps=TEST_STEPS,\n",
    "    callbacks=[log_callback, gen_callback, mc, es]\n",
    ")\n",
    "\n",
    "minutes_elapsed += (time() - start_time) // 60\n",
    "epochs_elapsed += EPOCHS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convert gpu model to cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'lstm/embedded_16_128md_e1_t2019-10-08T18:06:50.h5'\n",
    "model = K.models.load_model(os.path.join(models_base_path, model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.training.common.CUDNNLSTM_LSTM import cudnnlstm_to_lstm\n",
    "\n",
    "cpu_model = cudnnlstm_to_lstm(model)\n",
    "cpu_model.compile(\n",
    "    loss='mean_squared_error',\n",
    "    optimizer='adam', \n",
    "    metrics=[\"mean_squared_error\"],\n",
    ")\n",
    "\n",
    "K.models.save_model(cpu_model, os.path.join(models_base_path, model_path).replace('.h5', '_cpu.h5'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
